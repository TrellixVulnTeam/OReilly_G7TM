{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [tensorflow estimator tutorials - model optimization](https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/Tutorial%20-%20TensorFlow%20Model%20Optimisation%20for%20Serving%20-%20MNIST%20with%20Keras.ipynb).\n",
    "\n",
    "This notebooks shows how to optimize the TensorFlow exported SavedModel by **shrinking** its size (to have less memory and disk footprints), and **improving** prediction latency. This can be accopmlished by applying the following:\n",
    "* **Freezing**: That is, converting the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph.\n",
    "* **Pruning**: That is, stripping unused nodes during the prediction path of the graph, merging duplicate nodes, as well as removing other node ops like summary, identity, etc.\n",
    "* **Quantization**:  That is, converting any large float Const op into an eight-bit equivalent, followed by a float conversion op so that the result is usable by subsequent nodes.\n",
    "* **Other refinements**: That includes constant folding, batch_norm folding, fusing convolusion, etc.\n",
    "\n",
    "The optimization operations we apply in this example are from the TensorFlow [Graph Conversion Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_constants), which is a c++ command-line tool. We use the Python APIs to call the c++ libraries. \n",
    "\n",
    "The Graph Transform Tool is designed to work on models that are saved as GraphDef files, usually in a binary protobuf format. However, the model exported after training and estimator is in SavedModel format (saved_model.pb file + variables folder with variables.data-* and variables.index files). \n",
    "\n",
    "We need to optimize the model and keep it the SavedModel format. Thus, the optimisation steps will be:\n",
    "1. Freeze the SavedModel: SavedModel -> GraphDef\n",
    "2. Optimize the freezed model: GraphDef -> GraphDef\n",
    "3. Convert the optimised freezed model to SavedModel: GraphDef -> SavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering and Vizualizing the Data\n",
    "\n",
    "Before we can build our model let's gather the data and spot check the first few examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from lib.utils import show_images\n",
    "import tensorflow as tf\n",
    "\n",
    "(train_data, train_labels), (eval_data, eval_labels) = tf.keras.datasets.mnist.load_data()\n",
    "NUM_CLASSES = 10\n",
    " \n",
    "show_images(train_data[:12], cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "The model for this vision task of classfiying digits will be a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network). Since we are more concerned with optimizing the model for performance we wont worry too much about the accuracy and loss metrics at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model_fn(params):\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28), name='input_image')\n",
    "    input_layer = tf.keras.layers.Reshape(target_shape=(28, 28, 1), name='reshape')(inputs)\n",
    "    \n",
    "    # convolutional layers\n",
    "    conv_inputs = input_layer\n",
    "    for i in range(params.num_conv_layers): \n",
    "        filters = params.init_filters * (2**i)\n",
    "        conv = tf.keras.layers.Conv2D(kernel_size=3, filters=filters, strides=1, padding='SAME', activation='relu')(conv_inputs)\n",
    "        max_pool = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='SAME')(conv)\n",
    "        batch_norm = tf.keras.layers.BatchNormalization()(max_pool)\n",
    "        conv_inputs = batch_norm\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten(name='flatten')(conv_inputs)\n",
    "    \n",
    "    # fully-connected layers\n",
    "    dense_inputs = flatten\n",
    "    for i in range(len(params.hidden_units)):\n",
    "        dense = tf.keras.layers.Dense(units=params.hidden_units[i], activation='relu')(dense_inputs)\n",
    "        dropout = tf.keras.layers.Dropout(params.dropout)(dense)\n",
    "        dense_inputs = dropout\n",
    "        \n",
    "    # softmax classifier\n",
    "    logits = tf.keras.layers.Dense(units=NUM_CLASSES, name='logits')(dense_inputs)\n",
    "    softmax = tf.keras.layers.Activation('softmax', name='softmax')(logits)\n",
    "\n",
    "    # keras model\n",
    "    model = tf.keras.models.Model(inputs, softmax)\n",
    "    return model\n",
    "\n",
    "def create_estimator(params, run_config): \n",
    "    keras_model = keras_model_fn(params)\n",
    "    print(keras_model.summary())\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params.learning_rate)\n",
    "    keras_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    mnist_classifier = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=keras_model,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    return mnist_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training and Evaluation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, run_config):  \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"input_image\": train_data},\n",
    "            y=train_labels[:,np.newaxis],\n",
    "            batch_size=params.batch_size,\n",
    "            num_epochs=None,\n",
    "            shuffle=True),\n",
    "        max_steps=params.max_training_steps\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"input_image\": eval_data},\n",
    "            y=eval_labels[:,np.newaxis],\n",
    "            batch_size=params.batch_size,\n",
    "            num_epochs=1,\n",
    "            shuffle=False),\n",
    "        steps=None,\n",
    "        throttle_secs=params.eval_throttle_secs\n",
    "    )\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_estimator(params, run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_LOCATION = '/saved_models/'\n",
    "MODEL_NAME = 'mnist'\n",
    "model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
    "\n",
    "params  = tf.contrib.training.HParams(\n",
    "    batch_size=100,\n",
    "    hidden_units=[512, 512],\n",
    "    num_conv_layers=3, \n",
    "    init_filters=64,\n",
    "    dropout=0.2,\n",
    "    max_training_steps=50,\n",
    "    eval_throttle_secs=10,\n",
    "    learning_rate=1e-3,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    save_checkpoints_steps=1000,\n",
    "    keep_checkpoint_max=3,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(model_dir):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "estimator = run_experiment(params, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serving_input_receiver_fn():\n",
    "    inputs = {'input_image': tf.placeholder(shape=[None,28,28], dtype=tf.float32, name='serving_input_image')}\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
    "\n",
    "export_dir = os.path.join(model_dir, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=make_serving_input_receiver_fn()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "saved_models_base=/saved_models/mnist/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "echo ${saved_model_dir}\n",
    "ls ${saved_model_dir}\n",
    "saved_model_cli show --dir=${saved_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy SavedModel to TensorFlow Serving Path\n",
    "\n",
    "TensorFlow serving reads models from a specified directory when the TensorFlow Serving process is launched. Below we are copying the saved model to a directory we can serve models from. The folder hierarchy is as follows\n",
    "```\n",
    "top level dir/\n",
    "    model_name/\n",
    "       version_number/\n",
    "            saved_model.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "saved_models_base=/saved_models/mnist/export/\n",
    "saved_model_dir=${saved_models_base}$(ls ${saved_models_base} | tail -n 1)\n",
    "\n",
    "mkdir -p /serving/mnist/1\n",
    "cp -r ${saved_model_dir}/* /serving/mnist/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve MNIST Model\n",
    "## Run TensorFlow Serving\n",
    "\n",
    "To create a TensorFlow Serving instance:\n",
    "1. Open a new terminal tab here in the Jupyter Lab UI.\n",
    "2. Make sure to switch the shell to bash by running `bash`.\n",
    "2. Use the following command to start the server.\n",
    "\n",
    "```sh\n",
    "tensorflow_model_server \\\n",
    "    --rest_api_port=8501 \\\n",
    "    --model_name=mnist \\\n",
    "    --model_base_path=/serving/mnist/ \\\n",
    "    --enable_batching\n",
    "```\n",
    "\n",
    "Now let's verify that the server is up and running via the curl command in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8501/v1/models/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the SavedModel Graph\n",
    "\n",
    "Before we optimize our model for performance we should get a sense for the size and composition of the TensorFlow graph. Keep these numbers in mind as we take steps to decrease the size of our model and increase its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import describe_graph, get_graph_def_from_saved_model, get_size\n",
    "\n",
    "saved_model_dir = os.path.join(\n",
    "    export_dir, [f for f in os.listdir(export_dir) if f.isdigit()][0])\n",
    "\n",
    "describe_graph(get_graph_def_from_saved_model(saved_model_dir))\n",
    "\n",
    "get_size(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference via REST Client\n",
    "\n",
    "We are ready to start sending images to our model for inference. To begin let's send a single image to make sure everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_URL = \"http://localhost:8501\"\n",
    "\n",
    "# Display the image we will classify as a test\n",
    "show_images(eval_data[:1])\n",
    "\n",
    "def predict(instances, version=1):\n",
    "    payload = {'instances': instances}\n",
    "    res = requests.post(\"{}/v1/models/mnist/versions/{}:predict\".format(SERVER_URL, version), json=payload)\n",
    "    res.raise_for_status()\n",
    "    response = res.json()\n",
    "    class_ids = [np.argmax(item) for item in response[\"predictions\"]]\n",
    "\n",
    "    return class_ids\n",
    "\n",
    "print(\"Class Prediction: {}\".format(predict([eval_data[0].tolist()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(version=1, batch=100, repeat=10):\n",
    "    instances = []\n",
    "    for i in range(batch):\n",
    "        img = eval_data[i]\n",
    "        instances.append(img.tolist())\n",
    "\n",
    "    #warmup request\n",
    "    predict(instances[:1], version=version)\n",
    "    print('Warm up request performed!')\n",
    "    print('Timer started...')\n",
    "\n",
    "    time_start = datetime.utcnow()\n",
    "    output = None\n",
    "\n",
    "    for i in range(repeat):\n",
    "        output = predict(instances, version=version)\n",
    "\n",
    "    time_end = datetime.utcnow()\n",
    "\n",
    "    time_elapsed_sec = (time_end - time_start).total_seconds()\n",
    "\n",
    "    print(\"Inference elapsed time: {} seconds\\n\".format(time_elapsed_sec))\n",
    "\n",
    "    print(\"Prediction produced for {} instances batch, repeated {} times\".format(len(output), repeat))\n",
    "    print(\"Average latency per batch: {} seconds\".format(time_elapsed_sec/repeat))\n",
    "\n",
    "    print(\"Prediction output for the last instance: {}\".format(output[0]))\n",
    "\n",
    "benchmark_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze SavedModel\n",
    "\n",
    "This function will convert the SavedModel into a GraphDef file (freezed_model.pb), and storing the variables as constant to the freezed_model.pb\n",
    "\n",
    "You need to define the graph output nodes for freezing. We are only interested in the output of **softmax/Softmax** node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import get_graph_def_from_file\n",
    "\n",
    "def freeze_graph(saved_model_dir):\n",
    "    from tensorflow.python.tools import freeze_graph\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    output_graph_filename = os.path.join(saved_model_dir, \"freezed_model.pb\")\n",
    "    output_node_names = \"softmax/Softmax\"\n",
    "    initializer_nodes = \"\"\n",
    "\n",
    "    freeze_graph.freeze_graph(\n",
    "        input_saved_model_dir=saved_model_dir,\n",
    "        output_graph=output_graph_filename,\n",
    "        saved_model_tags = tag_constants.SERVING,\n",
    "        output_node_names=output_node_names,\n",
    "        initializer_nodes=initializer_nodes,\n",
    "\n",
    "        input_graph=None, \n",
    "        input_saver=False,\n",
    "        input_binary=False, \n",
    "        input_checkpoint=None, \n",
    "        restore_op_name=None, \n",
    "        filename_tensor_name=None, \n",
    "        clear_devices=False,\n",
    "        input_meta_graph=False,\n",
    "    )\n",
    "    \n",
    "    print(\"SavedModel graph freezed!\")\n",
    "    \n",
    "freeze_graph(saved_model_dir)\n",
    "\n",
    "freezed_filepath=os.path.join(saved_model_dir,'freezed_model.pb')\n",
    "describe_graph(get_graph_def_from_file(freezed_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Freeze Graph to SavedModel\n",
    "\n",
    "In order to serve the freeze graph we created we need to convert it back into a saved model. You should notice the variable size go to 0 when we run get_size at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_def_to_saved_model(graph_filepath, export_dir):\n",
    "    from tensorflow.python import ops\n",
    "\n",
    "    if tf.gfile.Exists(export_dir):\n",
    "        tf.gfile.DeleteRecursively(export_dir)\n",
    "\n",
    "    graph_def = get_graph_def_from_file(graph_filepath)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "        tf.saved_model.simple_save(session,\n",
    "                export_dir,\n",
    "                inputs={\n",
    "                    node.name: session.graph.get_tensor_by_name(\"{}:0\".format(node.name)) \n",
    "                    for node in graph_def.node if node.op=='Placeholder'},\n",
    "                outputs={\n",
    "                    \"softmax\": session.graph.get_tensor_by_name(\"softmax/Softmax:0\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(\"Optimised graph converted to SavedModel!\")\n",
    "        \n",
    "convert_graph_def_to_saved_model(freezed_filepath, \"/serving/mnist/2\")\n",
    "\n",
    "get_size(\"/serving/mnist/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8501/v1/models/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark the Frozen Model\n",
    "\n",
    "Be sure to compare the performance here to the performance in from the first version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_inference(version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the freezed_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_graph(model_dir, graph_filename, transforms):\n",
    "    from tensorflow.tools.graph_transforms import TransformGraph\n",
    "    \n",
    "    input_names = []\n",
    "    output_names = ['softmax/Softmax']\n",
    "    \n",
    "    graph_def = get_graph_def_from_file(os.path.join(model_dir, graph_filename))\n",
    "    optimised_graph_def = TransformGraph(graph_def, \n",
    "                                         input_names,\n",
    "                                         output_names,\n",
    "                                         transforms \n",
    "                                        )\n",
    "    tf.train.write_graph(optimised_graph_def,\n",
    "                        logdir=model_dir,\n",
    "                        as_text=False,\n",
    "                        name='optimised_model.pb')\n",
    "    \n",
    "    print(\"Freezed graph optimised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    'remove_nodes(op=Identity)', \n",
    "    'fold_constants(ignore_errors=true)',\n",
    "    'fold_batch_norms',\n",
    "    'fuse_resize_pad_and_conv',\n",
    "    'quantize_weights',\n",
    "    'quantize_nodes',\n",
    "    'merge_duplicate_nodes',\n",
    "    'strip_unused_nodes', \n",
    "    'sort_by_execution_order'\n",
    "]\n",
    "\n",
    "optimize_graph(saved_model_dir, 'freezed_model.pb', transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the Optimised Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "describe_graph(get_graph_def_from_file(optimised_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Optimised graph (GraphDef) to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_filepath=os.path.join(saved_model_dir,'optimised_model.pb')\n",
    "describe_graph(get_graph_def_from_file(optimised_filepath))\n",
    "\n",
    "convert_graph_def_to_saved_model(optimised_filepath, \"/serving/mnist/3\")\n",
    "get_size(\"/serving/mnist/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark the Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_inference(version=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
